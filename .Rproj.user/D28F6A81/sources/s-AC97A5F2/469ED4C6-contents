# WE HAVE TO DISCUSS MORE THAT THE GENERATION OF THE DATA IN THE TRAIN COULD DIFFER
# FROM THAT IN THE TEST, WITHOUT INCOURRING INTO MISSPECIFICATION ISSUES 
# THE EXAMPLE BELOW COMPARES OUR APPROACH WITH THAT OF THE REVIEWER (MISSING DATA)
# IN SUCH A CONTEXT. WE DO WELL 

# ALSO, 2 LINK FUNCTIONS SHOULD BE ALLOWED IN THE MIXTURE OF EXPERTS PART
# AND IT SHOULD BE SHOWN THAT AVERAGING OUT IS WORST THAN DOING 2 STEPS IN PREDICTION

# ALSO MODEL SHOULD BE PHRASED AS ONE, AS IT IS


library(truncnorm)
#oldpar <- par(no.readonly =TRUE)

#-- Simulate intervention study biomarker and food quantity data --#

P <- D <- 3; n <- 50
alpha <- rtruncnorm(P, 0, Inf, 4, 1)
beta <- rtruncnorm(P, 0, Inf, 0.001, 0.1)
x <- c(50, 100, 150)
labels_z <- sample(c(1,2,3), n, replace = TRUE)
quantities <- x[labels_z]
sigma_d <- 30
z <- rtruncnorm(n, 0, Inf, x[labels_z], sigma_d)
Y <- sapply( 1:P, function(p) sapply( 1:n, function(i)
  max(0, alpha[p] + beta[p]*z[i] + rnorm( 1, 0, 14) ) ) )

#-- Simulate Biomarker data only --#
nNew <- 20
x2 <- c(30, 70, 120, 180)
labels_zNew <- sample(c(1,2,3,4), nNew, replace = TRUE)
zNew <- rtruncnorm(nNew, 0, Inf, x2[labels_zNew], sigma_d)
YNew <- sapply( 1:P, function(p) sapply( 1:nNew, function(i)
  max(0, alpha[p] + beta[p]*zNew[i] + rnorm( 1, 0, 14) ) ) )

par(mfrow =c(1,2))
boxplot( z ~ x[labels_z])
boxplot( zNew ~ x2[labels_zNew])

par(mfrow=c(2,2))
boxplot( Y[,1] ~ x[labels_z])
boxplot( Y[,2] ~ x[labels_z])
boxplot( Y[,3] ~ x[labels_z])


#-- Fit the multiMarker model to the intervention study data --#
# Number of iterations (and burnIn) set small for example.
modM <- multiMarker(y = Y, quantities = quantities,
                    niter = 10000, burnIn = 30,
                    posteriors = TRUE)
# niter and burnIn values are low only for example purposes

# Number of iterations (and burnIn) set small for example.
predM <- predict(modM, y = YNew, niter = 10000, burnIn = 30,
                 posteriors = TRUE)


mario <- lm( x[labels_z] ~ Y)
m2 <- mario$coefficients[1] + mario$coefficients[2]*YNew[,1]
+ mario$coefficients[3]*YNew[,2]  + mario$coefficients[4]*YNew[,3]

mean(abs(m2 -zNew))

library(pls)
ppp <- plsr( x[labels_z] ~ Y)
ppp2 <- apply(predict(ppp, newdata = YNew),c(1,2), mean)
mean(abs(ppp2 -zNew))


mean(abs(predM$predictions$ZPred_P[1,] -zNew))


par(mfrow =c(2,2))
plot(predM$predictions$ZPred_P[1,] -zNew, pch = 19, ylim = c(-200, 200))
abline(h= 0)
plot(ppp2 -zNew, pch = 19, ylim = c(-200, 200))
abline(h= 0)
plot(m2 -zNew, pch = 19, ylim = c(-200, 200))
abline(h= 0)
par(mfrow =c(1,2))
boxplot( z ~ x[labels_z])
boxplot( zNew ~ x2[labels_zNew])

plot(predM$predictions$ZPred_P[1,], zNew, pch=19,asp=1)
abline(0,1)
plot(ppp2, zNew, pch=19, asp=1)
abline(0,1)

#####################
library(mclust)
adjustedRandIndex(apply(predM$predictions$Prob_P[,,1],1,which.max), labels_zNew)

#########JAGS
library(runjags)
Y3 <- rbind(Y, YNew)
dat.jags <- list(Y = Y3, c = c(labels_z, rep(NA, 20)),
                 N = 70, P = P, D = D, x = x)
# Fit JAGS model ---------------------------------------------------------------
mod.jags <- "
model{
for(i in 1:N) {
for(p in 1:P) {
Y[i,p] ~ dnorm(alpha[p] + beta[p]*z[i], 1/var_y[p]) T(0,)
}
c[i] ~ dcat(pi[]) # only observed for first 90 observations
z[i] ~ dnorm(mu_z[c[i]], 1/var_z[c[i]]) T(0,)
}
# Mixture model parameters
for(d in 1:D) {
mu_z[d] <- x[d]
sd_z[d] ~ dgamma(1e-3,1e-3)
var_z[d] <- pow(sd_z[d],2)
}
# Priors
pi ~ ddirich(rep(1,D))
for(p in 1:P) {
alpha[p] ~ dnorm(0,1e-2) T(0,)
beta[p] ~ dnorm(0,1e-2) T(0,)
sd_y[p] ~ dgamma(1e-3,1e-3)
var_y[p] <- pow(sd_y[p],2)
}
z_pred <- z[51:70] # Food intake levels for the predictions
c_pred <- c[51:70] # Food intake labels for the predictions
}"

monitor.var <- c("alpha", "beta", "pi", "var_z", "z_pred", "c_pred")
fit.jags <- run.jags(mod.jags, n.chains = 1, sample = 5000, thin = 1,
                     monitor = monitor.var, burnin = 1000, adapt = 100,
                     data = dat.jags)

sumjags <- summary(fit.jags)

adjustedRandIndex(sumjags[33:52, 2], labels_zNew)
mean(abs(sumjags[13:32, 2]-zNew))
plot(zNew, sumjags[13:32, 2])
